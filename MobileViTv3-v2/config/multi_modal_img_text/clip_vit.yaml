taskname: '+ CLIP-ViT-B/16'
common:
  run_label: "train"
  log_freq: 500
  auto_resume: true
  mixed_precision: true
  accum_freq: 1
  save_all_checkpoints: true
  save_interval_freq: 5000
dataset:
  root_train: "/mnt/img_text_tar/training"
  root_val: ""
  name: "img_text_tar"
  # effective batch size is > 65536 as we use multi-scale variable-batch sampler
  # 65k = (32 nodes * 8 gpus per node * 256 batches per GPU)
  train_batch_size0: 256
  val_batch_size0: 4
  eval_batch_size0: 4
  padding_index: 0
  persistent_workers: false
  pin_memory: true
  workers: 16
  category: "multi_modal_img_text"
  multi_modal_img_text:
    context_length: 77
    img_text_tar:
      metadata_file: "PATH_OF_METADATA_FILE"
      parallel_download: true
      s3_bucket_path: "DATASET_LOC_ON_S3_BUCKET"
    zero_shot_eval: true
    zero_shot:
      name: "imagenet"
      root: "mnt/imagenet/validation"
text_tokenizer:
  name: "clip"
  clip:
    merges_path: "http://download.pytorch.org/models/text/clip_merges.bpe"
    encoder_json_path: "http://download.pytorch.org/models/text/clip_encoder.json"
image_augmentation:
  random_resized_crop:
    enable: true
    interpolation: "bilinear"
    scale: [0.9, 1.0]
  resize:
    enable: true
    size: 224 # shorter size is 224
    interpolation: "bilinear"
  center_crop:
    enable: true
    size: 224
sampler:
  name: "variable_batch_sampler"
  use_shards: true
  vbs:
    crop_size_width: 224
    crop_size_height: 224
    max_n_scales: 5
    min_crop_size_width: 160
    max_crop_size_width: 320
    min_crop_size_height: 160
    max_crop_size_height: 320
    check_scale: 32
loss:
  category: "multi_modal_image_text"
  multi_modal_image_text:
    name: "contrastive_loss_clip_with_na"
  neural_aug:
    perceptual_metric: "psnr"
    target_value: [ 40, 20 ]
    curriculum_method: "cosine"
optim:
  name: "adamw"
  weight_decay: 0.2
  no_decay_bn_filter_bias: true
  adamw:
    beta1: 0.9
    beta2: 0.98
    eps: 1.e-6
scheduler:
  is_iteration_based: true
  max_epochs: 12
  max_iterations: 150000
  name: cosine
  warmup_init_lr: 1.e-06
  warmup_iterations: 1000
  cosine:
    max_lr: 0.0005
    min_lr: 1.e-06
model:
  multi_modal_image_text: # multi-modal image-text model
    name: "clip"
    lr_multiplier_img_encoder: 1.0
    lr_multiplier_text_encoder: 1.0
    clip:
      projection_dim: 512
  classification:
    name: "vit"
    gradient_checkpointing: true
    vit:
      mode: "base"
      norm_layer: "layer_norm_fp32"
      checkpoint_segments: 4
    activation:
      name: "gelu"
  image_projection_head:
    name: "simple_projection_nc2nc"
  text: # text encoder
    name: "transformer"
    transformer:
      causal_masking: true
      model_dim: 512
      no_scale_embedding: false
      no_pos_embedding: false
      embed_dropout: 0.0
      dropout: 0.0
      attn_dropout: 0.0
      ffn_dropout: 0.0
      n_transformer_layers: 12
      ffn_multiplier_per_layer: 4.0
      n_heads_per_layer: 8
      norm_layer: "layer_norm_fp32"
      gradient_checkpoint: true
  learn_augmentation:
    brightness: true
    contrast: true
    noise: true
    mode: "distribution"
  normalization:
    name: "batch_norm"
    momentum: 0.1
  activation:
    name: "gelu"
    inplace: true
  layer:
    global_pool: "mean"
    conv_init: "kaiming_uniform"
    linear_init: "trunc_normal"
    linear_init_std_dev: 0.02
ema:
  enable: true
  momentum: 0.0005
stats:
  val: [ "top1" ]
  train: ["loss", "grad_norm" ]
  checkpoint_metric: "top1.zero_shot_image_logits"
  checkpoint_metric_max: true
